# -*- coding: utf-8 -*-
"""Ops.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LqN0T0qUtgWDWJC6Oj9Ns1omvW2b4c21
"""

import torch
import torch.nn as nn
import pdb
import torch.nn.functional as F
import numpy as np, cv2 as cv, scipy
from scipy import signal
import collections
import tensorflow as tf
from tensorflow.python.ops import summary_op_util


def preprocess(image):
    # Converts image range from [0,1] to [-1,1]
    return image * 2 - 1


def deprocess(image):
    # [-1,1] --> [0,1]
    return (image + 1) / 2


def preprocessLr(image):
    identity = nn.Identity()
    return identity(image)


def deprocessLr(image):
    identity = nn.Identity()
    return identity(image)


def conv2_tran(input_channels, kernel=3, output_channel=64, stride=1, use_bias=True, output_padding=0):
    padding = int((kernel - 1) / 2)

    if use_bias:
        trans_conv = nn.ConvTranspose2d(input_channels, output_channel, kernel, stride, padding=padding, bias=True
                                        , output_padding=output_padding)
    else:
        trans_conv = nn.ConvTranspose2d(input_channels, output_channel, kernel, stride, padding=padding, bias=False
                                        , output_padding=output_padding)
    return trans_conv


def conv2(batch_input, kernel=3, output_channels=64, stride=1, use_bias=True):
    padding = int((kernel - 1) / 2)
    if use_bias:
        conv = nn.Conv2d(batch_input, output_channels, kernel, stride, padding=padding, bias=True)
    else:
        conv = nn.Conv2d(batch_input, output_channels, kernel, stride, padding=padding, bias=False)
    return conv


def prelu(inputs):
    prelu = nn.PReLU(inputs.shape[1], 0)
    return prelu


def lrelu(alphas):
    return nn.LeakyReLU(negative_slope=alphas)


def batchnorm(inputs, is_training):
    batchn = nn.BatchNorm2d(inputs, eps=0.001)
    return batchn


def maxpool(kernel_size=(2, 2)):
    pool = nn.MaxPool2d(kernel_size)
    return pool


def denselayer(inputs, output_size):
    fc = nn.Linear(inputs, output_size)
    torch.nn.init.xavier_uniform_(fc.weight)
    return fc


def pixelshuffle(inputs, scale=2):
    shuffle = nn.PixelShuffel(2)
    return shuffle


def upscale_four(inputs):
    upsample = nn.Upsample(scale_factor=4, mode="bilinear")
    return upsample(inputs)


def bicubic_four(inputs):
    upsample = nn.Upsample(scale_factor=4, mode="bicubic")
    return upsample(inputs)


def phaseShift(inputs, scale, shape_1, shape_2):
    X = torch.reshape(inputs, shape_1)
    X = torch.transpose(X, [0, 1, 2, 3, 4])
    return torch.reshape(X, shape_2)


def random_flip_batch(input, decision):
    identity = torch.identity()
    f1 = identity(input)
    f2 = torch.flip(input, dim=3)
    return torch.where(torch.less(decision, 0.5), f2, f1)


def random_flip(input, decision):
    identity = torch.identity()
    f1 = identity(input)
    f2 = torch.flip(input, dim=3)
    return torch.where(torch.less(decision, 0.5), f2, f1)


def compute_psnr(ref, target):
    ref = ref.float()
    target = target.float()
    diff = target - ref
    sqr = torch.multiply(diff, diff)
    err = sqr.sum()
    v = diff.shape()[0] * diff.shape()[1] * diff.shape()[2] * diff.shape()[3]
    mse = err / v.float()
    psnr = 10. * (torch.log(255. * 255. / mse) / torch.log(10.))

    return psnr


class VGG19(nn.Module):
    def __init__(self):
        super(VGG19, self).__init__()
        self.Conv1_1 = nn.Sequential(nn.Conv2d(3, 64, 3, padding=1), nn.ReLU())
        self.Conv1_2 = nn.Sequential(nn.Conv2d(64, 64, 3, padding=1), nn.ReLU())
        self.pool1 = nn.MaxPool2d((2, 2), stride=2)
        self.Conv2_1 = nn.Sequential(nn.Conv2d(64, 128, 3, padding=1), nn.ReLU())
        self.Conv2_2 = nn.Sequential(nn.Conv2d(128, 128, 3, padding=1), nn.ReLU())
        self.pool2 = nn.MaxPool2d((2, 2), stride=2)
        self.Conv3_1 = nn.Sequential(nn.Conv2d(128, 256, padding=1), nn.ReLU())
        self.Conv3_2 = nn.Sequential(nn.Conv2d(256, 256, padding=1), nn.ReLU())
        self.Conv3_3 = nn.Sequential(nn.Conv2d(256, 256, padding=1), nn.ReLU())
        self.Conv3_4 = nn.Sequential(nn.Conv2d(256, 256, padding=1), nn.ReLU())
        self.pool3 = nn.MaxPool2d((2, 2), stride=2)
        self.Conv4_1 = nn.Sequential(nn.Conv2d(256, 512, padding=1), nn.ReLU())
        self.Conv4_2 = nn.Sequential(nn.Conv2d(512, 512, padding=1), nn.ReLU())
        self.Conv4_3 = nn.Sequential(nn.Conv2d(512, 512, padding=1), nn.ReLU())
        self.Conv4_4 = nn.Sequential(nn.Conv2d(512, 512, padding=1), nn.ReLU())
        self.pool4 = nn.MaxPool2d((2, 2), stride=2)
        self.Conv5_1 = nn.Sequential(nn.Conv2d(512, 512, padding=1), nn.ReLU())
        self.Conv5_2 = nn.Sequential(nn.Conv2d(512, 512, padding=1), nn.ReLU())
        self.Conv5_3 = nn.Sequential(nn.Conv2d(512, 512, padding=1), nn.ReLU())
        self.Conv5_4 = nn.Sequential(nn.Conv2d(512, 512, padding=1), nn.ReLU())
        self.pool5 = nn.MaxPool2d((2, 2), stride=2)
        self.end_points = {}

    def forward(self, x):
        x = self.Conv1_1(x)
        self.end_points["vgg_19/conv1_1"] = x
        x = self.Conv1_2(x)
        self.end_points["vgg_19/conv1_2"] = x
        x = self.pool1(x)
        self.end_points["vgg_19/pool1"] = x
        x = self.Conv2_1(x)
        self.end_points["vgg_19/conv2_1"] = x
        x = self.Conv2_2(x)
        self.end_points["vgg_19/conv2_2"] = x
        x = self.pool2(x)
        self.end_points["vgg_19/pool2"] = x
        x = self.Conv3_1(x)
        self.end_points["vgg_19/conv3_1"] = x
        x = self.Conv3_2(x)
        self.end_points["vgg_19/conv3_2"] = x
        x = self.Conv3_3(x)
        self.end_points["vgg_19/conv3_3"] = x
        x = self.Conv3_4(x)
        self.end_points["vgg_19/conv3_4"] = x
        x = self.pool3(x)
        self.end_points["vgg_19/pool3"] = x
        x = self.Conv4_1(x)
        self.end_points["vgg_19/conv4_1"] = x
        x = self.Conv4_2(x)
        self.end_points["vgg_19/conv4_2"] = x
        x = self.Conv4_3(x)
        self.end_points["vgg_19/conv4_3"] = x
        x = self.Conv4_4(x)
        self.end_points["vgg_19/conv4_4"] = x
        x = self.pool4(x)
        self.end_points["vgg_19/pool4"] = x
        x = self.Conv5_1(x)
        self.end_points["vgg_19/conv5_1"] = x
        x = self.Conv5_2(x)
        self.end_points["vgg_19/conv5_2"] = x
        x = self.Conv5_3(x)
        self.end_points["vgg_19/conv5_3"] = x
        x = self.Conv5_4(x)
        self.end_points["vgg_19/conv5_4"] = x
        output = self.pool5(x)
        self.end_points["vgg_19/pool5"] = output
        return output, self.end_points


def gaussian_2dkernel(size=5, sig=1.):
    """
    Returns a 2D Gaussian kernel array with side length size and a sigma of sig
    """
    gkern1d = signal.gaussian(size, std=sig).reshape(size, 1)
    gkern2d = np.outer(gkern1d, gkern1d)
    return (gkern2d / gkern2d.sum())


def torch_data_gaussDownby4(HRdata, sigma=1.5):
    """
    tensorflow version of the 2D down-scaling by 4 with Gaussian blur
    sigma: the sigma used for Gaussian blur
    return: down-scaled data
    """
    k_w = 1 + 2 * int(sigma * 3.0)
    gau_k = gaussian_2dkernel(k_w, sigma)
    gau_0 = np.zeros_like(gau_k)
    gau_list = np.float32([
        [gau_k, gau_0, gau_0],
        [gau_0, gau_k, gau_0],
        [gau_0, gau_0, gau_k]])  # only works for RGB images!
    gau_wei = np.transpose(gau_list, [2, 3, 0, 1])

    fix_gkern = tf.constant(gau_wei, dtype=tf.float32, shape=[k_w, k_w, 3, 3], name='gauss_blurWeights')
    # shape [batch_size, crop_h, crop_w, 3]
    cur_data = tf.nn.conv2d(HRdata, fix_gkern, strides=[1, 4, 4, 1], padding="VALID", name='gauss_downsample_4')
    cur_data = tf.make_ndarray(cur_data)
    cur_data = torch.from_numpy(cur_data).cuda()
    return cur_data


def load_ckpt(checkpoint, model):
    return model.load_state_dict(torch.load(checkpoint))


def encode_gif(images, fps):
    """Encodes numpy images into gif string.
    Args:
      images: A 5-D `uint8` `np.array` (or a list of 4-D images) of shape
        `[batch_size, time, height, width, channels]` where `channels` is 1 or 3.
      fps: frames per second of the animation
    Returns:
      The encoded gif string.
    Raises:
      IOError: If the ffmpeg command returns an error.
    """
    from subprocess import Popen, PIPE
    c, h, w = images[0].shape
    cmd = ['ffmpeg', '-y',
           '-f', 'rawvideo',
           '-vcodec', 'rawvideo',
           '-r', '%.02f' % fps,
           '-s', '%dx%d' % (w, h),
           '-pix_fmt', {1: 'gray', 3: 'rgb24'}[c],
           '-i', '-',
           '-filter_complex', '[0:v]split[x][z];[z]palettegen[y];[x][y]paletteuse',
           '-r', '%.02f' % fps,
           '-f', 'gif',
           '-']
    proc = Popen(cmd, stdin=PIPE, stdout=PIPE, stderr=PIPE)
    for image in images:
        proc.stdin.write(image.tostring())
    out, err = proc.communicate()
    if proc.returncode:
        err = '\n'.join([' '.join(cmd), err.decode('utf8')])
        raise IOError(err)
    del proc
    return out


def py_gif_summary(tag, images, max_outputs, fps):
    """Outputs a `Summary` protocol buffer with gif animations.
    Args:
      tag: Name of the summary.
      images: A 5-D `uint8` `np.array` of shape `[batch_size, time, height, width,
        channels]` where `channels` is 1 or 3.
      max_outputs: Max number of batch elements to generate gifs for.
      fps: frames per second of the animation
    Returns:
      The serialized `Summary` protocol buffer.
    Raises:
      ValueError: If `images` is not a 5-D `uint8` array with 1 or 3 channels.
    """
    is_bytes = isinstance(tag, bytes)
    if is_bytes:
        tag = tag.decode("utf-8")
    images = np.asarray(images)
    if images.dtype != np.uint8:
        raise ValueError("Tensor must have dtype uint8 for gif summary.")
    if images.ndim != 5:
        raise ValueError("Tensor must be 5-D for gif summary.")
    batch_size, _, height, width, channels = images.shape
    if channels not in (1, 3):
        raise ValueError("Tensors must have 1 or 3 channels for gif summary.")

    summ = tf.Summary()
    num_outputs = min(batch_size, max_outputs)
    for i in range(num_outputs):
        image_summ = tf.Summary.Image()
        image_summ.height = height
        image_summ.width = width
        image_summ.colorspace = channels  # 1: grayscale, 3: RGB
        try:
            image_summ.encoded_image_string = encode_gif(images[i], fps)
        except (IOError, OSError) as e:
            tf.logging.warning("Unable to encode images to a gif string because either ffmpeg is "
                               "not installed or ffmpeg returned an error: %s. Falling back to an "
                               "image summary of the first frame in the sequence.", e)
            try:
                from PIL import Image  # pylint: disable=g-import-not-at-top
                import io  # pylint: disable=g-import-not-at-top
                with io.BytesIO() as output:
                    Image.fromarray(images[i][0]).save(output, "PNG")
                    image_summ.encoded_image_string = output.getvalue()
            except:
                tf.logging.warning("Gif summaries requires ffmpeg or PIL to be installed: %s", e)
                image_summ.encoded_image_string = "".encode('utf-8') if is_bytes else ""
        if num_outputs == 1:
            summ_tag = "{}/gif".format(tag)
        else:
            summ_tag = "{}/gif/{}".format(tag, i)
        summ.value.add(tag=summ_tag, image=image_summ)
    summ_str = summ.SerializeToString()
    return summ_str


def gif_summary(name, tensor, max_outputs, fps, collections=None, family=None):
    """Outputs a `Summary` protocol buffer with gif animations.
    Args:
      name: Name of the summary.
      tensor: A 5-D `uint8` `Tensor` of shape `[batch_size, time, height, width,
        channels]` where `channels` is 1 or 3.
      max_outputs: Max number of batch elements to generate gifs for.
      fps: frames per second of the animation
      collections: Optional list of tf.GraphKeys.  The collections to add the
        summary to.  Defaults to [tf.GraphKeys.SUMMARIES]
      family: Optional; if provided, used as the prefix of the summary tag name,
        which controls the tab name used for display on Tensorboard.
    Returns:
      A scalar `Tensor` of type `string`. The serialized `Summary` protocol
      buffer.
    """
    tensor = tf.image.convert_image_dtype(tensor, dtype=tf.uint8, saturate=True)
    # tensor = tf.convert_to_tensor(tensor)
    if summary_op_util.skip_summary():
        return tf.constant("")
    with summary_op_util.summary_scope(name, family, values=[tensor]) as (tag, scope):
        val = tf.py_func(
            py_gif_summary,
            [tag, tensor, max_outputs, fps],
            tf.string,
            stateful=False,
            name=scope)
        summary_op_util.collect(val, collections, [tf.GraphKeys.SUMMARIES])
    return val


def save_img(out_path, img):
    img = np.clip(img * 255.0, 0, 255).astype(np.uint8)
    cv.imwrite(out_path, img[:, :, ::-1])
